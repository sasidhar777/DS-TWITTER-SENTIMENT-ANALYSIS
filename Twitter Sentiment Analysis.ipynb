{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21beb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9941563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified code from down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b25a252d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model:\n",
      "Accuracy: 0.5238095238095238\n",
      "Confusion Matrix:\n",
      " [[4 9]\n",
      " [1 7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.31      0.44        13\n",
      "           4       0.44      0.88      0.58         8\n",
      "\n",
      "    accuracy                           0.52        21\n",
      "   macro avg       0.62      0.59      0.51        21\n",
      "weighted avg       0.66      0.52      0.50        21\n",
      "\n",
      "SVM Model:\n",
      "Accuracy: 0.42857142857142855\n",
      "Confusion Matrix:\n",
      " [[ 1 12]\n",
      " [ 0  8]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.14        13\n",
      "           4       0.40      1.00      0.57         8\n",
      "\n",
      "    accuracy                           0.43        21\n",
      "   macro avg       0.70      0.54      0.36        21\n",
      "weighted avg       0.77      0.43      0.31        21\n",
      "\n",
      "KNN Model:\n",
      "Accuracy: 0.6666666666666666\n",
      "Confusion Matrix:\n",
      " [[7 6]\n",
      " [1 7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.54      0.67        13\n",
      "           4       0.54      0.88      0.67         8\n",
      "\n",
      "    accuracy                           0.67        21\n",
      "   macro avg       0.71      0.71      0.67        21\n",
      "weighted avg       0.75      0.67      0.67        21\n",
      "\n",
      "Decision Tree Model:\n",
      "Accuracy: 0.5238095238095238\n",
      "Confusion Matrix:\n",
      " [[4 9]\n",
      " [1 7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.31      0.44        13\n",
      "           4       0.44      0.88      0.58         8\n",
      "\n",
      "    accuracy                           0.52        21\n",
      "   macro avg       0.62      0.59      0.51        21\n",
      "weighted avg       0.66      0.52      0.50        21\n",
      "\n",
      "\n",
      "Random Forest Model:\n",
      "Accuracy: 0.5238095238095238\n",
      "Confusion Matrix:\n",
      " [[ 3 10]\n",
      " [ 0  8]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.23      0.38        13\n",
      "           4       0.44      1.00      0.62         8\n",
      "\n",
      "    accuracy                           0.52        21\n",
      "   macro avg       0.72      0.62      0.50        21\n",
      "weighted avg       0.79      0.52      0.47        21\n",
      "\n",
      "\n",
      " Adaboost Forest Model:\n",
      "Accuracy: 0.6190476190476191\n",
      "Confusion Matrix:\n",
      " [[5 8]\n",
      " [0 8]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.38      0.56        13\n",
      "           4       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.62        21\n",
      "   macro avg       0.75      0.69      0.61        21\n",
      "weighted avg       0.81      0.62      0.60        21\n",
      "\n",
      "\n",
      "Gradient Forest Model:\n",
      "Accuracy: 0.47619047619047616\n",
      "Confusion Matrix:\n",
      " [[ 3 10]\n",
      " [ 1  7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.23      0.35        13\n",
      "           4       0.41      0.88      0.56         8\n",
      "\n",
      "    accuracy                           0.48        21\n",
      "   macro avg       0.58      0.55      0.46        21\n",
      "weighted avg       0.62      0.48      0.43        21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Load the Twitter dataset\n",
    "data = pd.read_csv(\"twitter_new.csv\", encoding=\"ISO-8859-1\")\n",
    "data.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "# Data Preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# remove the plural form\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers, and extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "    text = \" \".join([word for word in text.split() if word.isalpha()]) \n",
    "    # Remove stopwords and convert to lowercase\n",
    "    text = \" \".join([word.lower() for word in text.split() if word not in stop_words])\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    #token = lemmatizer.lemmatize(token)\n",
    "    return text\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Feature Extraction using Bag of Words (BoW)\n",
    "vectorizer = CountVectorizer(max_features=5000) # top 5000 frequent words are selected \n",
    "X = vectorizer.fit_transform(data[\"text\"])\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Logistic Regression\n",
    "Logistic_Regression= LogisticRegression()\n",
    "Logistic_Regression.fit(X_train, y_train)\n",
    "\n",
    "# SVM Classifier\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "#KNN Classifier\n",
    "Knn_classifier = KNeighborsClassifier()\n",
    "Knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Decision Tree\n",
    "DecisionTree_classifier = DecisionTreeClassifier(criterion='entropy',max_depth=100)\n",
    "DecisionTree_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#  Adaboost Classifier\n",
    "Ab_classifier = AdaBoostClassifier()\n",
    "Ab_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Gradinet boost Classifier\n",
    "Gb_classifier = GradientBoostingClassifier()\n",
    "Gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    return accuracy, conf_matrix, class_report\n",
    "\n",
    "lr_accuracy, lr_conf_matrix, lr_class_report = evaluate_model(Logistic_Regression, X_test, y_test)\n",
    "svm_accuracy, svm_conf_matrix, svm_class_report = evaluate_model(svm_classifier, X_test, y_test)\n",
    "knn_accuracy, knn_conf_matrix, knn_class_report = evaluate_model(Knn_classifier, X_test, y_test)\n",
    "decision_tree_accuracy, decision_tree_conf_matrix,decision_tree_class_report = evaluate_model(DecisionTree_classifier, X_test, y_test)\n",
    "rf_accuracy, rf_conf_matrix, rf_class_report = evaluate_model(rf_classifier, X_test, y_test)\n",
    "ab_accuracy, ab_conf_matrix, ab_class_report = evaluate_model(Ab_classifier, X_test, y_test)\n",
    "gb_accuracy, gb_conf_matrix, gb_class_report = evaluate_model(Gb_classifier, X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Logistic Regression Model:\")\n",
    "print(\"Accuracy:\", lr_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", lr_conf_matrix)\n",
    "print(\"Classification Report:\\n\", lr_class_report)\n",
    "\n",
    "print(\"SVM Model:\")\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", svm_conf_matrix)\n",
    "print(\"Classification Report:\\n\", svm_class_report)\n",
    "\n",
    "print(\"KNN Model:\")\n",
    "print(\"Accuracy:\", knn_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", knn_conf_matrix)\n",
    "print(\"Classification Report:\\n\", knn_class_report)\n",
    "\n",
    "print(\"Decision Tree Model:\")\n",
    "print(\"Accuracy:\", decision_tree_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", decision_tree_conf_matrix)\n",
    "print(\"Classification Report:\\n\", decision_tree_class_report)\n",
    "\n",
    "\n",
    "print(\"\\nRandom Forest Model:\")\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", rf_conf_matrix)\n",
    "print(\"Classification Report:\\n\", rf_class_report)\n",
    "\n",
    "print(\"\\n Adaboost Forest Model:\")\n",
    "print(\"Accuracy:\", ab_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", ab_conf_matrix)\n",
    "print(\"Classification Report:\\n\", ab_class_report)\n",
    "\n",
    "print(\"\\nGradient Forest Model:\")\n",
    "print(\"Accuracy:\", gb_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", gb_conf_matrix)\n",
    "print(\"Classification Report:\\n\", gb_class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04356069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code ends \n",
    "# Knn has hihgest accuracy in predicting sentiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993313d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58aa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffa3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf4c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Twitter dataset\n",
    "data = pd.read_csv(\"path_to_dataset.csv\", encoding=\"ISO-8859-1\")\n",
    "data.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "# Data Preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers, and extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "    text = \" \".join([word for word in text.split() if word.isalpha()]) \n",
    "    # Remove stopwords and convert to lowercase\n",
    "    text = \" \".join([word.lower() for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Feature Extraction using Bag of Words (BoW)\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data[\"text\"])\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classification model (Random Forest)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f7d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c014947a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d7cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c76443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"twitter_new.csv\", encoding=\"ISO-8859-1\")\n",
    "df.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "rows_with_target_0 = df[df['target'] == 0]\n",
    "rows_with_target_4 = df[df['target'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746ea4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4]\n"
     ]
    }
   ],
   "source": [
    "distinct_values = df['target'].unique()\n",
    "\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6b8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=rows_with_target_0.loc[0:50]\n",
    "df2 =rows_with_target_4.loc[799999:800050]\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11e871c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>1467824377</td>\n",
       "      <td>Mon Apr 06 22:23:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Toyia1</td>\n",
       "      <td>I am bored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4</td>\n",
       "      <td>1467824422</td>\n",
       "      <td>Mon Apr 06 22:23:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>deeper2k</td>\n",
       "      <td>@stevecla it is a wallpaper with Red Square I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4</td>\n",
       "      <td>1467824454</td>\n",
       "      <td>Mon Apr 06 22:23:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>vmdavinci</td>\n",
       "      <td>@r_keith_hill Thanks for your response. Ihad a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>4</td>\n",
       "      <td>1467824474</td>\n",
       "      <td>Mon Apr 06 22:23:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Missy_BB</td>\n",
       "      <td>@Pope_Mello Well yeah! I only noticed cuz i'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>4</td>\n",
       "      <td>1467824495</td>\n",
       "      <td>Mon Apr 06 22:23:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>EmCDL</td>\n",
       "      <td>Just tryna get inspired thats all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target         ids                          date      flag   \n",
       "0         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  \\\n",
       "1         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "..      ...         ...                           ...       ...   \n",
       "98        4  1467824377  Mon Apr 06 22:23:18 PDT 2009  NO_QUERY   \n",
       "99        4  1467824422  Mon Apr 06 22:23:19 PDT 2009  NO_QUERY   \n",
       "100       4  1467824454  Mon Apr 06 22:23:20 PDT 2009  NO_QUERY   \n",
       "101       4  1467824474  Mon Apr 06 22:23:20 PDT 2009  NO_QUERY   \n",
       "102       4  1467824495  Mon Apr 06 22:23:20 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "1         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "2          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "3           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "4         joy_wolf                      @Kwesidei not the whole crew   \n",
       "..             ...                                                ...  \n",
       "98          Toyia1                                        I am bored   \n",
       "99        deeper2k  @stevecla it is a wallpaper with Red Square I ...  \n",
       "100      vmdavinci  @r_keith_hill Thanks for your response. Ihad a...  \n",
       "101       Missy_BB  @Pope_Mello Well yeah! I only noticed cuz i'm ...  \n",
       "102          EmCDL                 Just tryna get inspired thats all   \n",
       "\n",
       "[103 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7eba88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd01f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c112a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a158f6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214e1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dde31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Twitter dataset\n",
    "data = pd.read_csv(\"output.csv\", encoding=\"ISO-8859-1\")\n",
    "data.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "#data = data.loc[0:1000000]\n",
    "# Data Preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers, and extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "    text = \" \".join([word for word in text.split() if word.isalpha()]) \n",
    "    # Remove stopwords and convert to lowercase\n",
    "    text = \" \".join([word.lower() for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Feature Extraction using Bag of Words (BoW)\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data[\"text\"])\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classification model (Random Forest)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
